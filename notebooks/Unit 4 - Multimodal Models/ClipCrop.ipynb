{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30587,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/royZhouroy/learngit/blob/main/notebooks/Unit%204%20-%20Multimodal%20Models/ClipCrop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLIP and CROP\n",
        "\n",
        "**Zero shot cropping of sections from images**"
      ],
      "metadata": {
        "id": "6gSJPl5AbpTM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this below tutorial, we have attempted to extract cropped out sections from image using the checkpoints of CLIP and YOLOS models hosted on HuggingFace hub. Generally to crop out sections from an image we need to manually select the section, but here we have attempted to extract sections just by using text descriptions.\n",
        "\n",
        "[**CLIP:**](https://huggingface.co/docs/transformers/model_doc/clip)\n",
        "\n",
        "CLIP is a multi-modal model that leverages the semantics between an image and a textual description. CLIP has a lot of extended applications apart from simple image classification. It is even the catalyst to the famous image generation model DALL-E. We can extend CLIP to object detection, object tracking, image ranking, similarity search, and many more depending on our creativity etc. CLIP is a huge breakthrough as it paved way for a lot of research in the multi-modal domain of AI.\n",
        "\n",
        "[**YOLOS:**](https://huggingface.co/docs/transformers/model_doc/yolos)\n",
        "\n",
        "YOLOS - You only look at one sequence, an adoptation of YOLO on the plain Vision Transformer (ViT) for object detection, inspired by DETR. It is seen that YOLOS-Base directly adopted from BERT-Base architecture can obtain 42.0 box AP on COCO object detection benchmark."
      ],
      "metadata": {
        "id": "YF_PNpk2bpTN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the modules"
      ],
      "metadata": {
        "id": "5QePDEXnbpTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import warnings\n",
        "import requests\n",
        "import PIL\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from transformers import CLIPProcessor, CLIPModel, YolosImageProcessor, YolosForObjectDetection"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-12T14:13:39.517048Z",
          "iopub.execute_input": "2023-12-12T14:13:39.517640Z",
          "iopub.status.idle": "2023-12-12T14:14:00.899331Z",
          "shell.execute_reply.started": "2023-12-12T14:13:39.517589Z",
          "shell.execute_reply": "2023-12-12T14:14:00.897967Z"
        },
        "trusted": true,
        "id": "CE6ydECtbpTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.simplefilter(\"ignore\")\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-12T14:14:00.901256Z",
          "iopub.execute_input": "2023-12-12T14:14:00.902806Z",
          "iopub.status.idle": "2023-12-12T14:14:00.911204Z",
          "shell.execute_reply.started": "2023-12-12T14:14:00.902765Z",
          "shell.execute_reply": "2023-12-12T14:14:00.907210Z"
        },
        "trusted": true,
        "id": "ZWkZWDncbpTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation"
      ],
      "metadata": {
        "id": "rYHJcnuFbpTP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model loading"
      ],
      "metadata": {
        "id": "QvdCwrUIbpTP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code blocks downloads the relevant model checkpoints from HuggingFace hub (YOLOS and CLIP respectively for object detection and semantic matching of images and text descriptions)"
      ],
      "metadata": {
        "id": "P_R_iZWebpTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DETECTION_MODEL_NAME = \"hustvl/yolos-tiny\"\n",
        "MULTIMODAL_MODEL_NAME = \"openai/clip-vit-base-patch16\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-12T14:15:50.503568Z",
          "iopub.execute_input": "2023-12-12T14:15:50.504041Z",
          "iopub.status.idle": "2023-12-12T14:15:50.511762Z",
          "shell.execute_reply.started": "2023-12-12T14:15:50.504008Z",
          "shell.execute_reply": "2023-12-12T14:15:50.510117Z"
        },
        "trusted": true,
        "id": "7FADdBjMbpTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading YOLOS model**"
      ],
      "metadata": {
        "id": "QUSPbYfUbpTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "det_image_processor = YolosImageProcessor.from_pretrained(DETECTION_MODEL_NAME)\n",
        "det_model = YolosForObjectDetection.from_pretrained(DETECTION_MODEL_NAME)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-12T14:15:52.755838Z",
          "iopub.execute_input": "2023-12-12T14:15:52.756247Z",
          "iopub.status.idle": "2023-12-12T14:15:58.635056Z",
          "shell.execute_reply.started": "2023-12-12T14:15:52.756217Z",
          "shell.execute_reply": "2023-12-12T14:15:58.633493Z"
        },
        "trusted": true,
        "id": "TK95thchbpTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading the CLIP model**"
      ],
      "metadata": {
        "id": "tGc83bIgbpTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mm_model = CLIPModel.from_pretrained(MULTIMODAL_MODEL_NAME)\n",
        "mm_processor = CLIPProcessor.from_pretrained(MULTIMODAL_MODEL_NAME)"
      ],
      "metadata": {
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2023-12-12T14:15:58.638650Z",
          "iopub.execute_input": "2023-12-12T14:15:58.639282Z",
          "iopub.status.idle": "2023-12-12T14:16:27.688761Z",
          "shell.execute_reply.started": "2023-12-12T14:15:58.639235Z",
          "shell.execute_reply": "2023-12-12T14:16:27.686975Z"
        },
        "trusted": true,
        "id": "fXzFQTEpbpTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementation using loaded modules from HuggingFace hub"
      ],
      "metadata": {
        "id": "TjLZJsq3bpTR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code blocks dowloads an image and loads it as a PIL Image"
      ],
      "metadata": {
        "id": "lqxfV8pTbpTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://i.pinimg.com/736x/1b/51/42/1b5142c269f2e9a356202af3f5569a87.jpg\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-12T14:16:54.476488Z",
          "iopub.execute_input": "2023-12-12T14:16:54.476869Z",
          "iopub.status.idle": "2023-12-12T14:16:54.637121Z",
          "shell.execute_reply.started": "2023-12-12T14:16:54.476840Z",
          "shell.execute_reply": "2023-12-12T14:16:54.635965Z"
        },
        "trusted": true,
        "id": "KRGO7kG3bpTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code blocks implementing functions responsible for cropping and extraction of images.**\n",
        "\n",
        "* The *extract_list_images_detected* function detects the objects with confidence scores above the given threshold\n",
        "* The *extract_image_with_description* function semantically matches with the list of extracted images and descriptions"
      ],
      "metadata": {
        "id": "e3hm1VatbpTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_list_images_detected(image, prob):\n",
        "    \"\"\"\n",
        "    The function `extract_list_images_detected` takes an image and a probability threshold as input,\n",
        "    performs object detection on the image using a pre-trained model, and returns a list of cropped\n",
        "    images of the detected objects along with their corresponding scores.\n",
        "\n",
        "    Args:\n",
        "    :param image: The `image` parameter is the input image from which you want to extract the detected\n",
        "    images. It should be a PIL image object\n",
        "    :param prob: The parameter \"prob\" is the probability threshold for filtering out the detected\n",
        "    objects. It is used to determine which objects are considered significant enough to be included in\n",
        "    the final list of images. Objects with a probability score higher than the threshold will be\n",
        "    included, while objects with a lower score will be filtered out\n",
        "\n",
        "    Returns:\n",
        "    images_list: returns a list of cropped out images.\n",
        "    scores: returns a list of confidence scores corresponding scores.\n",
        "    \"\"\"\n",
        "\n",
        "    inputs = det_image_processor(images=image, return_tensors=\"pt\")\n",
        "    outputs = det_model(**inputs)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    bboxes = outputs.pred_boxes\n",
        "    probas = outputs.logits.softmax(-1)[0, :, :-1]\n",
        "\n",
        "    keep = probas.max(-1).values > prob\n",
        "    outs = det_image_processor.post_process(outputs, torch.tensor(image.size[::-1]).unsqueeze(0))\n",
        "    bboxes_scaled = outs[0]['boxes'][keep].detach().numpy()\n",
        "    labels = outs[0]['labels'][keep].detach().numpy()\n",
        "    scores = outs[0]['scores'][keep].detach().numpy()\n",
        "\n",
        "    images_list = []\n",
        "    for i,j in enumerate(bboxes_scaled):\n",
        "        xmin = int(j[0])\n",
        "        ymin = int(j[1])\n",
        "        xmax = int(j[2])\n",
        "        ymax = int(j[3])\n",
        "\n",
        "        im_arr = np.array(image)\n",
        "        roi = im_arr[ymin:ymax, xmin:xmax]\n",
        "        roi_im = Image.fromarray(roi)\n",
        "        images_list.append(roi_im)\n",
        "\n",
        "    return images_list, scores"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-12T14:16:55.094804Z",
          "iopub.execute_input": "2023-12-12T14:16:55.096513Z",
          "iopub.status.idle": "2023-12-12T14:16:55.110923Z",
          "shell.execute_reply.started": "2023-12-12T14:16:55.096454Z",
          "shell.execute_reply": "2023-12-12T14:16:55.109380Z"
        },
        "trusted": true,
        "id": "WSNHgMQobpTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_image_with_description(images_list, text, scores):\n",
        "    \"\"\"\n",
        "    The function `extract_image_with_description` takes a list of images, a text description, and\n",
        "    scores, and returns the image with the highest score.\n",
        "\n",
        "    Args:\n",
        "    images_list: The `images_list` parameter is a list of images. Each image can be represented\n",
        "    as a file path or a URL\n",
        "    text: The `text` parameter is a string that represents the description or caption of the\n",
        "    image\n",
        "    scores: The \"scores\" parameter is a list of scores corresponding to each image in the\n",
        "    \"images_list\". These scores represent the confidence or relevance of each image to the given text\n",
        "\n",
        "    Returns:\n",
        "    the image with the highest score (PIL.Image) and its corresponding score.\n",
        "    \"\"\"\n",
        "\n",
        "    input_ = mm_processor(text = [text], images=images_list , return_tensors=\"pt\", padding=True)\n",
        "    output = mm_model(**input_)\n",
        "    logits_per_image = output.logits_per_text\n",
        "    probs = logits_per_image.softmax(-1)\n",
        "    l_idx = np.argsort(probs[-1].detach().numpy())[::-1][0:1]\n",
        "\n",
        "    final_ims = []\n",
        "    for i,j in enumerate(images_list):\n",
        "        param_dict = {}\n",
        "        if i in l_idx:\n",
        "            param_dict['image'] = images_list[i]\n",
        "            param_dict['score'] = probs[-1].detach().numpy()[i]\n",
        "            final_ims.append(param_dict)\n",
        "\n",
        "    fi = sorted(final_ims, key=lambda item: item.get(\"score\"), reverse=True)\n",
        "    return fi[0]['image'], fi[0]['score']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-12T14:16:55.748062Z",
          "iopub.execute_input": "2023-12-12T14:16:55.749331Z",
          "iopub.status.idle": "2023-12-12T14:16:55.763227Z",
          "shell.execute_reply.started": "2023-12-12T14:16:55.749260Z",
          "shell.execute_reply": "2023-12-12T14:16:55.761447Z"
        },
        "trusted": true,
        "id": "ZgqICGaFbpTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Object Detection using YOLOS"
      ],
      "metadata": {
        "id": "5uBDxAC-bpTS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gets the cropped list of sections of image along with their confidence scores"
      ],
      "metadata": {
        "id": "cGE5Xn61bpTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_rois, conf_scores = extract_list_images_detected(image, prob=0.85)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-12T14:16:57.231595Z",
          "iopub.execute_input": "2023-12-12T14:16:57.232030Z",
          "iopub.status.idle": "2023-12-12T14:16:58.551609Z",
          "shell.execute_reply.started": "2023-12-12T14:16:57.231997Z",
          "shell.execute_reply": "2023-12-12T14:16:58.549964Z"
        },
        "trusted": true,
        "id": "x7IG3SytbpTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extraction using CLIP"
      ],
      "metadata": {
        "id": "9gC2qHHkbpTT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Viewing the input image using matplotlib"
      ],
      "metadata": {
        "id": "QKZIGmrIbpTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(image)\n",
        "plt.axis(\"on\")\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-12T14:17:02.511583Z",
          "iopub.execute_input": "2023-12-12T14:17:02.512262Z",
          "iopub.status.idle": "2023-12-12T14:17:02.997293Z",
          "shell.execute_reply.started": "2023-12-12T14:17:02.512223Z",
          "shell.execute_reply": "2023-12-12T14:17:02.996251Z"
        },
        "trusted": true,
        "id": "M0_Q3o3dbpTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code blocks implementing semantic matching of images with provided descriptions. We provide a list of descriptions for which semantically matched images will be extracted and appended to a dictionary with relevant confidence scores."
      ],
      "metadata": {
        "id": "EiYdL3pFbpTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT_DESC = [\"Man carrying a green bag\", \"man riding a bicycle\", \"yellow colored taxi car\", \"red colored bus\"]\n",
        "images_list = []\n",
        "for txt in TEXT_DESC:\n",
        "    img_dict = {}\n",
        "    img, score = extract_image_with_description(list_rois, txt, conf_scores)\n",
        "    img_dict['image'] = img\n",
        "    img_dict['description'] = txt\n",
        "    img_dict['conf-score'] = score\n",
        "    images_list.append(img_dict)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-12T14:17:03.519838Z",
          "iopub.execute_input": "2023-12-12T14:17:03.520643Z",
          "iopub.status.idle": "2023-12-12T14:17:46.471116Z",
          "shell.execute_reply.started": "2023-12-12T14:17:03.520600Z",
          "shell.execute_reply": "2023-12-12T14:17:46.469306Z"
        },
        "trusted": true,
        "id": "5bDs1WjvbpTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(10, 20))\n",
        "for i in range(len(images_list)):\n",
        "    plt.subplot(1, len(images_list), i + 1)\n",
        "    plt.imshow(images_list[i]['image'])\n",
        "    plt.title(images_list[i]['description'])\n",
        "    plt.xlabel(\"confidence-score:\" + str(round(images_list[i]['conf-score'], 3)))\n",
        "    plt.xticks(ticks=[])\n",
        "    plt.yticks(ticks=[])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-12T14:17:46.474189Z",
          "iopub.execute_input": "2023-12-12T14:17:46.474697Z",
          "iopub.status.idle": "2023-12-12T14:17:47.338279Z",
          "shell.execute_reply.started": "2023-12-12T14:17:46.474658Z",
          "shell.execute_reply": "2023-12-12T14:17:47.337018Z"
        },
        "trusted": true,
        "id": "EWTFGyQvbpTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "It can be seen that most of the cases could be really efficient as expected, but the drop in accuracy could be visible in other scenarios due to the fact we are leveraging smaller models for inference."
      ],
      "metadata": {
        "id": "axYVYhAhbpTU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6UMVBhzQbpTU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}